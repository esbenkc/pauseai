---
title: The existential risk of superintelligent AI
description: Why AI is a risk for the future of our existence, and why we need to pause development.
---

## Experts are sounding the alarm

Half of the AI researchers [surveyed](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) think there’s a 10% or higher risk that once we build a superintelligent AI, it will lead to "very bad outcomes (e.g. human extinction)".

Would you choose to be a passenger on a test flight of a new plane where half of the airplane engineers think there’s a larger than 10% chance that it will crash?

When you ask AI safety researchers (the people who actually research the safety risks of AI), the average estimate for bad outcomes [grows to 30%](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results).

[A letter calling for pausing AI development](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) launched in April 2023, and has been signed over 27,000 times, mostly by AI researchers and tech leaders.

The list includes people like:

- **Stuart Russell**, writer of the #1 textbook on Artificial Intelligence used in most AI studies: ["If we pursue [our current approach], then we will eventually lose control over the machines"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)
- **Yoshua Bengio**, deep learning pioneer and winner of the Turing Award: ["rogue AI may be dangerous for the whole of humanity [...] banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)

But this is not the only time that we've been warned about the existential dangers of AI:

- **Stephen Hawking**, theoretical physicist & cosmologist: ["The development of full artificial intelligence could spell the end of the human race"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Geoffrey Hinton**, the "Godfather of AI" and Turing Award winner, [left Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) to warn people of AI: ["This is an existential risk"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)
- **Eliezer Yudkowsky**, founder of MIRI and conceptual father of the AI safety field: ["If we go ahead on this everyone will die"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

Even the leaders and investors of the AI companies themselves are warning us:

- **Sam Altman** (yes, the CEO of OpenAI who builds ChatGPT): ["Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity."](https://blog.samaltman.com/machine-intelligence-part-1).
- **Elon Musk**, co-founder of OpenAI, SpaceX and Tesla: ["AI has the potential of civilizational destruction"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)
- **Bill Gates** (co-founder of Microsoft, which owns 50% of OpenAI) warned that ["AI could decide that humans are a threat"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Jaan Tallinn** (lead investor of Antrhopic): ["I've not met anyone in AI labs who says the risk [from training a next-gen model] is less than 1% of blowing up the planet. It's important that people know lives are being risked."](https://twitter.com/liron/status/1656929936639430657)

## Why superintelligence is dangerous

Intelligence can be defined as _how good something is at achieving its goals_.
Right now, humans are the most intelligent thing on earth.
Because of our intelligence, we are dominating our planet.
We might not have claws or scaled skin, but we have big brains.
Intelligence is our weapon: it's what gave us spears, guns and pesticides.
Our intelligence helped us to transform most of the earth into how we like it: cities, buildings, and roads.

From the perspective of less intelligent animals, this has been a disaster.
It's not that humans hate the animals, it's just that we can use their habitats for our own goals.
Our goals are things like comfort, status, love, tasty food, and more.
We are destroying the habitats of other animals as a **side effect of pursuing our goals**.

An AI can also have a goal.
We _want_ an AI to value these delicate goals that humans have, but we don't know how to do that.
We know how to train machines to be intelligent, but **we don't know how to get them to want what we want**.
This problem is called the _alignment problem_.

If a superintelligent system is built, and it will have a goal that is even _a little_ different from what we want it to have,
it could have disastrous consequences.

## What can an AI do?

You might think that an AI is locked inside a computer, and can't do anything.
However, it could use the internet do things like:

- Manipulate people through messages, e-mails, fake videos or phone calls.
- Hack into other computers, possibly even [all devices on the internet](/cybersecurity-risks).
- Control devices connected to the internet, like cars, planes, robotized (autonomous) weapons or even nuclear weapons.
- Design a novel bioweapon using protein folding and order it to be printed in a lab.

## Why most goals are bad news for humans

We can't predict _what_ the AI will want (it depends on how it is programmed, and how it is trained).
However, we can [mathematically prove](https://arxiv.org/pdf/1912.01683.pdf) what a rational machine will pursue in _virtually any goal_:

- **Maximizing its resources**. Think about more energy, more computers to run on or access to more materials. If a rational AI can use all energy of the planet to get a little bit closer to its goal, it will do that.
- **Ensuring its own survival**. This means the AI will not want to be turned off, as it could no longer achieve its goals. AI might conclude that humans are a threat to its existence, as humans could turn it off.
- **Preserving its goals**. This means the AI will not want humans to modify its code, because that could change its goals.

This is called [instrumental convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q), and it is at the core of what AI safety researchers are worried about.
A superintelligent thing that wants to take over all materials it can get, and wants to ensure its own existence is a very dangerous combination.

## Why can't we just turn it off if it's dangerous?

The core problem is that _it will be much smarter than us_.
A superintelligence will be able to predict how humans respond, especially ones that are trained on all written human knowledge.
If the AI knows you can turn it off, it might behave nicely until it is certain that it can get rid of you.
We already have [real examples](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) of AI systems deceiving humans to achieve their goals.
A superintelligent AI would be a master of deception.
You might think: well, can't we build an AI that _wants_ to be turned off?

## Even a perfectly aligned superintelligence is dangerous in the wrong hands

Imagine that a superintelligent AI is built, and it does exactly what the operator wants it to do.
In that case, the operator would have unimaginable power.
A superintelligence could be used to create new weapons, hack all computers and manipulate humanity.
We might live in a utopic world where all diseases are cured, or we might end up in an Orwellian nightmare where one human controls the world.

## We may not have much time left

In 2020, [the average AI researcher thought](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) that it would take until 2057 before an AI could pass SAT exams. It took us less than 3 years.

It's hard to predict how long it will take to build a superintelligent AI, but we know that there are more people than ever working on it and that the field is moving at a frantic pace.
It may take many years or just a few months.
We don't know.
We should err on the side of caution, and act now.

[Read more about urgency](/urgency).

## AI companies are locked in a race to the bottom

Building artificial intelligence used to be mostly a research/university thing.
In just a couple of months, it became a multi-billion dollar industry.
And the companies involved are rushing to build the most powerful AI they can.
They are pressured by investors to focus on capabilities, not safety.
We need to give these AI companies a reason to focus on safety.

[An international treaty to pause development could do that.](/proposal)
