---
title: FAQ
description: Frequently asked questions about PauseAI and the risks of superintelligent AI.
---

### Who are you?

We are a group of volunteers, AI (safety) researchers and engineers who are worried about the risks of AI.
We are not affiliated with any company or organization.
You can [find us on Discord](https://discord.gg/2XXWXvErfA).

### How likely is it that superintelligent AI will cause very bad outcomes, like human extinction?

AI safety researchers (who are the experts on this topic) are divided on this question, and estimates [range from 2% to 97% with an average of 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Note that no (surveyed) AI safety researchers believe that there's a 0% chance.
However, there might be selection bias here: people who work in the AI safety field are likely to do so because they believe preventing bad AI outcomes is important.

If you ask AI researchers in general (not safety specialists), this number drops to a [mean value of around 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/).
A small minority, about 4% of them, believe that the alignment problem is not a real problem.
Note that there might be a selection bias here in the opposite direction: people who work in AI are likely to do so because they believe AI will be beneficial.

_Imagine you're invited to take a test flight on a new airplane_.
The airplane safety experts on average think there's a 30% chance it would crash.
The plane engineers think there's a 14% chance of crashing.
About 4% of engineers are saying that there's no chance of crashing, it is uncrashable.

Would you enter that plane? Because right now, we're all boarding the AI plane.

### How long do we have until superintelligent AI?

It might take months, it might take decades, nobody knows for sure.
However, we do know that the pace of AI progress is often grossly underestimated.
Just three years ago we thought we'd have SAT-passing AI systems in 2055.
We got there in April 2023.
We should act as if we have very little time left because we don't want to be caught off guard.

[Read more about urgency](/urgency).

### OpenAI and Google are saying they want to be regulated. Why are you protesting them?

We applaud [OpenAI](https://openai.com/blog/governance-of-superintelligence) and [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) for their calls for international regulation of AI.
However, we believe that the current proposals are not enough to prevent an AI catastrophe.
Google and Microsoft have not yet publicly stated anything about the existential risk of AI.
Only OpenAI [explicitly mentions the risk of extinction](https://openai.com/blog/governance-of-superintelligence), and again we applaud them for taking this risk seriously.
However, their strategy is quite explicit: a Pause is impossible, we need to get to superintelligence first.
The problem with this, however, is that they [do not believe they have solved the alignment problem](https://youtu.be/L_Guz73e6fw?t=1478).
The AI companies are locked in a race to the bottom, where AI safety is sacrificed for competitive advantage.
This is simply the result of market dynamics.
We need governments to step in and organize a summit to discuss the risks of AI, and to create a regulatory framework that will prevent the worst outcomes.

### Aren't you just scared of changes and new technology?

You might be surprised that most people in PauseAI consider themselves techno-optimists.
Many of them are involved in AI development, are gadget lovers, and have mostly been very excited about the future.
Particularly many of them have been excited about the potential of AI to help humanity.
That's why for many of us the sad realization that AI might be an existential risk was a very difficult one to internalize.

### I have a different / AI related question

Try [AIsafety.info](https://aisafety.info/), an awesome database of questions and answers about AI safety.
